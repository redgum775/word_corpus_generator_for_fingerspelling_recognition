{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jaconv\n",
    "from pykakasi import kakasi\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = re.compile('[\\u3041-\\u309Fー]{2,10}')  # （全角ひらがなのUnicodeブロック + 伸ばし棒）の正規表現"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# プログレスバー\n",
    "class ProgressBar:\n",
    "  def __init__(self, max_id):\n",
    "    self.call = 0\n",
    "    self.max_id = max_id\n",
    "  \n",
    "  def show(self, id, comment=''):\n",
    "    if self.call == 0:\n",
    "      self.call = 1\n",
    "      print(f'{int(id/self.max_id*100): >3}%|{\"■\"*int((id/self.max_id)*20)+\"_\"*(20 - int((id/self.max_id)*20))}| {id}/{self.max_id}:{comment:128}', end=\"\")\n",
    "    elif id >= self.max_id:\n",
    "      print(f'\\r{int(id/self.max_id*100): >3}%|{\"■\"*int((id/self.max_id)*20)+\"_\"*(20 - int((id/self.max_id)*20))}| {id}/{self.max_id}:{comment:128}')\n",
    "    else:\n",
    "      print(f'\\r{int(id/self.max_id*100): >3}%|{\"■\"*int((id/self.max_id)*20)+\"_\"*(20 - int((id/self.max_id)*20))}| {id}/{self.max_id}:{comment:128}', end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 郵便データベースの読み込み\n",
    "postal_database = '../datasets/japanesepost/KEN_ALL_utf8.CSV'\n",
    "postal_df = pd.read_csv(postal_database, header=None, encoding='utf-8')\n",
    "prefecture = set(postal_df[3][:])   # 都道府県名（半角カナ）\n",
    "municipality = set(postal_df[4][:]) # 市町村名 （半角カナ）\n",
    "# address = set(postal_df[5][:])    # 町域名（半角カナ）\n",
    "\n",
    "# データの重複をなくす\n",
    "postal_corpus = set([])\n",
    "postal_corpus |= prefecture\n",
    "postal_corpus |= municipality \n",
    "# postal_corpus |= address\n",
    "\n",
    "# （半角カナ -> 全角ひらがな）に変換\n",
    "# ひらがな以外（数字・記号など）が含まれる文字列を除外\n",
    "formatted_postal_corpus =[]\n",
    "for c in postal_corpus:\n",
    "  c = jaconv.hankaku2zenkaku(c) # 半角 -> 全角\n",
    "  c = jaconv.kata2hira(c)       # カタカナ -> ひらがな\n",
    "  if p.fullmatch(c):            # 正規表現チェック\n",
    "    formatted_postal_corpus.append(c)\n",
    "\n",
    "print(f'Postal corpus total: {len(formatted_postal_corpus)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 日本語能力試験\n",
    "noryoku_database = '../datasets/noryoku/noryoku.txt'\n",
    "noryoku_df = pd.read_csv(noryoku_database, encoding='utf-8',sep='\\t')\n",
    "noryoku_corpus = set(noryoku_df['語'][:])\n",
    "\n",
    "formatted_noryoku_corpus =[]\n",
    "for c in noryoku_corpus:\n",
    "  c = jaconv.hankaku2zenkaku(c) # 半角 -> 全角\n",
    "  c = jaconv.kata2hira(c)       # カタカナ -> ひらがな\n",
    "  if p.fullmatch(c):            # 正規表現チェック\n",
    "    formatted_noryoku_corpus.append(c)\n",
    "\n",
    "print(f'Noryoku corpus total: {len(formatted_noryoku_corpus)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文化庁\n",
    "bunkacho_database = '../datasets/bunkacho/katakana_shiyou.txt'\n",
    "bunkacho_df = pd.read_csv(bunkacho_database, encoding='utf-8',sep='\\t')\n",
    "bunkacho_corpus = set(bunkacho_df['語'][:])\n",
    "\n",
    "formatted_bunkacho_corpus =[]\n",
    "for c in bunkacho_corpus:\n",
    "  c = jaconv.hankaku2zenkaku(c) # 半角 -> 全角\n",
    "  c = jaconv.kata2hira(c)       # カタカナ -> ひらがな\n",
    "  if p.fullmatch(c):            # 正規表現チェック\n",
    "    formatted_bunkacho_corpus.append(c)\n",
    "\n",
    "print(f'Bunkacho corpus total: {len(formatted_bunkacho_corpus)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openBDから本のISBNとタイトルのリストを取得\n",
    "import time\n",
    "import requests\n",
    "from more_itertools import chunked\n",
    "\n",
    "OPENBD_ENDPOINT = 'https://api.openbd.jp/v1/'\n",
    "JA_BOOK_CODE = '9784'\n",
    "DATA = 10000\n",
    "\n",
    "# 全てのISBNコードを取得\n",
    "def get_coverage():\n",
    "  return requests.get(OPENBD_ENDPOINT + 'coverage').json()\n",
    "\n",
    "# \n",
    "def get_bibs(items) -> dict:\n",
    "  return requests.post(OPENBD_ENDPOINT + 'get', data={'isbn': ','.join(items)}).json()\n",
    "\n",
    "# 全てのISBNコードを取得\n",
    "all_coverage = get_coverage()\n",
    "\n",
    "# 全てのISBNコードうち日本の書籍コードだけ抽出\n",
    "filter_coverage = list(filter(lambda c: c[:4]==JA_BOOK_CODE, all_coverage))\n",
    "\n",
    "# ISBNのリストを10000件単位に分割\n",
    "chunked_coverage = chunked(filter_coverage, DATA)\n",
    "\n",
    "print(f'ISBN total: {len(filter_coverage)} items')\n",
    "\n",
    "openBD_f = open('../datasets/openBD/book_title.csv', 'w', encoding='utf-8', newline=\"\")\n",
    "openBD_writer = csv.writer(openBD_f)\n",
    "openBD_writer.writerow(['id','isbn','title'])\n",
    "\n",
    "# ISBNとタイトルをcsvに記録\n",
    "count = 0\n",
    "start = time.time()\n",
    "progress_bar = ProgressBar(max_id=len(filter_coverage))\n",
    "for coverage in chunked_coverage:\n",
    "  result = get_bibs(coverage)\n",
    "  for bib in result:\n",
    "    count += 1\n",
    "    progress_bar.show(count)  # プログレスバー\n",
    "\n",
    "    if bib is None:\n",
    "      continue\n",
    "    # ここで書誌1件単位の処理\n",
    "    isbn = bib['summary']['isbn']\n",
    "    title = bib['summary']['title']\n",
    "    openBD_writer.writerow([f'{count}', f'{isbn}', f'{title}'])\n",
    "\n",
    "openBD_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# タイトルが10文字以下の書籍をリスト化\n",
    "openBD_database = '../datasets/openBD/book_title.csv'\n",
    "openBD_df = pd.read_csv(openBD_database, encoding='utf-8')\n",
    "title_list = openBD_df['title']\n",
    "book_title_corpus = []\n",
    "kks = kakasi()\n",
    "progress_bar = ProgressBar(len(title_list))\n",
    "for idx, title in enumerate(title_list):\n",
    "  progress_bar.show(idx+1, title)  # プログレスバー\n",
    "  if title != title:  # NaNチェック\n",
    "    continue\n",
    "  result = kks.convert(title)  # 漢字 -> ひらがな\n",
    "  title = ''.join([item['hira'] for item in result])\n",
    "  title = jaconv.hankaku2zenkaku(title) # 半角 -> 全角\n",
    "  title = jaconv.kata2hira(title)       # カタカナ -> ひらがな\n",
    "  if p.fullmatch(title):            # 正規表現チェック\n",
    "    book_title_corpus.append(title)\n",
    "print(f'OpenBD corpus total: {len(book_title_corpus)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# タイトルが10文字以下の書籍リストを保存\n",
    "f = open('../datasets/openBD/book_title.txt', 'w', encoding='utf-8')\n",
    "for title in book_title_corpus:\n",
    "  f.write(title+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正規表現[あ-んー]をLと定めた時、Lの二つ組LLのリストを作成\n",
    "KANA = ['あ', 'い', 'う', 'え', 'お',\n",
    "        'か', 'が', 'き', 'ぎ', 'く', 'ぐ', 'け', 'げ', 'こ', 'ご',\n",
    "        'さ', 'ざ', 'し', 'じ', 'す', 'ず', 'せ', 'ぜ', 'そ', 'ぞ',\n",
    "        'た', 'だ', 'ち', 'ぢ', 'っ', 'つ', 'づ', 'て', 'で', 'と', 'ど',\n",
    "        'な', 'に', 'ぬ', 'ね', 'の',\n",
    "        'は', 'ば', 'ぱ', 'ひ', 'び', 'ぴ', 'ふ', 'ぶ', 'ぷ', 'へ', 'べ', 'ぺ', 'ほ', 'ぼ', 'ぽ',\n",
    "        'ま', 'み', 'む', 'め', 'も',\n",
    "        'ゃ', 'や', 'ゅ', 'ゆ', 'ょ', 'よ',\n",
    "        'ら', 'り', 'る', 'れ', 'ろ',\n",
    "        'わ', 'を', 'ん', 'ー']\n",
    "LL_LIST = []\n",
    "for first in KANA:\n",
    "  for second in KANA:\n",
    "    LL_LIST.append(f'{first}{second}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語とLLのリストをcsv形式で出力\n",
    "f = open('../output/words.csv', 'w', encoding='utf-8')\n",
    "write = csv.writer(f)\n",
    "\n",
    "formatted_corpus = set(formatted_postal_corpus + formatted_noryoku_corpus + formatted_bunkacho_corpus + book_title_corpus)\n",
    "write.writerow(['単語']+LL_LIST)  # header\n",
    "progress_bar = ProgressBar(len(formatted_corpus))\n",
    "for idx, word in enumerate(formatted_corpus):\n",
    "  progress_bar.show(idx+1, word)\n",
    "  if word != word:\n",
    "    continue\n",
    "  isextend_ll = []\n",
    "  for ll in LL_LIST:\n",
    "    if ll in word:\n",
    "      isextend_ll.append(1)\n",
    "    else: \n",
    "      isextend_ll.append(0)\n",
    "  write.writerow([word] + isextend_ll)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語とLLのリストを読み込む\n",
    "dtype_dict = dict(zip(['単語']+LL_LIST, ['object']+['bool']*len(LL_LIST)))  # 読み取るときの型定義\n",
    "corpus_df = pd.read_csv('../output/words.csv', dtype=dtype_dict, encoding='utf-8')\n",
    "corpus_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_pattern_count = len(LL_LIST)\n",
    "ll_cover_rate = (corpus_df[LL_LIST].sum()[corpus_df[LL_LIST].sum()[:]!=0].count() / len(LL_LIST))*100\n",
    "print(f'LL pattern: {ll_pattern_count} ways\\nLL cover rate: {ll_cover_rate: .1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語コーパスを保存\n",
    "f = open('../output/words.txt', 'w', encoding='utf8')\n",
    "for word in sorted(corpus_df['単語']):\n",
    "  f.write(word+'\\n')\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bf013d35b017588c7d7e71f9a994c2c562f09e2e9412870a8ef22129528f5b72"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
